\section{Related Work}
\label{sec:related_work}
In the following sections, a literature review is conducted. The first two parts describe the current scientific approaches to measure emotions for both faces and voices using deep neural networks. The third part explains the relationship between the quality of presentations and emotions.

\subsection{Facial Emotion Recognition}
\label{subsec:related_work_facial_emotion_recognition}
Facial Emotion Recognition (FER) is a subarea of computer vision and deals with the prediction of human emotional states using facial mimics and expressions in images, moving pictures or videos \cite{jain_extended_2019}.
According to \citeA{rosler_reducing_2021} FER literature and research suggest two separate ways of feature generation: manually or automatically through a deep neural network. Additionally, they outline the differentiation of their underlying emotional model, which is either based on discrete emotional states or continuous dimensions \cite{rosler_reducing_2021}. Five fundamental discrete emotional states have been identified by \citeA{ekman_universal_1997}: Happiness, sadness, fear/surprise, disgust, and anger. Several different pieces of literature are recognizing other fundamental emotional states, and thus a single definition is not available. The continuous dimensions are defined differently. For instance, \citeA{mollahosseini_affectnet_2017} use two or three dimensions to explain emotions, with valence or pleasantness as one dimension and arousal or activation as the other.

In this work, we will focus on leveraging discrete emotional states since it is more maturely researched and more suitable to generate features using deep learning-based approaches. Considering the number of developments of recent years in deep learning, we concentrate on using the available variety of deep neural networks as the most appropriate technique in FER \cite{jain_extended_2019}. We could also focus on FER approaches that use handcrafted features and which are according to \citeA{ko_brief_2018} often deployed in the following three steps. First, face and facial component detection from the input image. Second, spatial and temporal feature extraction. And third, expression classification by e.g. Support Vector Machine or Random Forests to recognize emotional states. Although this manual way often leads to accurate results and requires less computational resources, many types of research, such as \citeA{jung_joint_2015}, showed the superiority of deep learning algorithms with neural network architectures like CNNs over handcrafted approaches.

One of the main advantages of CNNs and other neural networks is the possibility to automatically learn features from the input images, which is called ``end-to-end'' learning \cite{ko_brief_2018}. This Moody Prototype also uses a CNN implementation via the \texttt{face-api.js} library, which is described further in Section~\ref{subsec:method_facial_emotion_recognition}. These in the context of facial emotion recognition most used CNNs operate and process images, as the name discloses, ``convolutionally'' and can consider spatial information \cite{ko_brief_2018, rosler_reducing_2021}. Thus, we utilize a CNN to recognize the emotional states of \citeA{ekman_universal_1997}: happy, sad, fearful, angry, surprised, and disgusted. Additionally, since audiences listen and their faces appear often simply neutrally, we added another emotional state to our recognition model, called ``neutral''.

\subsection{Vocal Emotion Recognition}
\label{subsec:related_work_vocal_emotion_recognition}
The use of emotional impulses that portray emotion in a single modality, typically through facial expressions, has become popular in emotion research. Emotional communication in the natural world, on the other hand, is temporal and multimodal. Multisensory integration is important while processing effective cues, according to research \cite{livingstone_ryerson_2018}. Researchers have created their own multimodal stimuli in the absence of proven multimodal sets (Livingstone and Russo 2018). Researchers have also combined two different unimodal sets \cite{delle-vigne_subclinical_2014} or joined self-created stimuli with an existing unimodal set \cite{zvyagintsev_attention_2013} to create multimodal stimuli. As each set differs in features, technical quality, and expressive intensity, comparing findings across studies may be difficult. As a consequence, differences in results could be attributed in part to differences in stimulus sets \cite{livingstone_ryerson_2018}.

In their research paper, \citeA{livingstone_ryerson_2018} describe the creation and validation of the RAVDESS, a dataset of dynamic and multimodal emotional emotions. The RAVDESS dataset has several key features that make it ideal for scientists, engineers, and physicians to use: It is provided freely available under a Creative Commons non-commercial license and is spoken by professional actors from North America. It has a variety of emotional reactions at two levels of emotional intensity \cite{livingstone_ryerson_2018}. 

The RAVDESS was validated with 247 raters from across North America. The accuracy with which participants properly identified the actors' intended emotions was referred to as validity. As is customary in the literature, \citeA{livingstone_ryerson_2018} looked at proportion correct scores. Overall, the results were excellent, with an average of 80\% for audio-video, 75\% for video-only, and 60\% for audio-only \cite{livingstone_ryerson_2018}.

Next to the RAVDESS dataset, there exist other voice datasets to train CNNs for voice emotion Recognition. The JL-Corpus is a set of four New Zealand English speakers who each say 15 sentences in each of five basic emotions with two repetitions, and another 10 sentences in each of five secondary emotions (An Open Source Emotional Speech Corpus for Human Robot Interaction Applications).

Another example is the Toronto Emotion Speech Set (TESS). It includes sentences of 200 words,  which always have the same structure \cite{pichora-fuller_toronto_2020}. The carrier phrase ``Say the word ...'' was spoken and recorded by two actresses (aged 26 and 64), and these were each divided into seven different emotions (anger, disgust, fear, happiness, pleasant surprise, sadness and neutral) \cite{pichora-fuller_toronto_2020}. Together, this resulted in 2800 different stimuli. The two actresses are from the Toronto area and their mother language is English \cite{pichora-fuller_toronto_2020}. The EMO-DB database is a free emotional database. The Institute of Communication Science, Technical University of Berlin, Germany, established the database. The datasets consist of the voices of ten German professional speakers (five males and five females), which display seven different emotions (anger, boredom,  anxiety, happiness, sadness, disgust, and neutral). There are 800 sentences recorded and seven emotions in the EMO-DB database \cite{burkhardt_database_2005}.

We use the four named datasets to train our own voice emotion CNN. The total combined amount of audio snippets from all datasets is 6216.

\subsection{Presentations and Emotions}
\label{subsec:related_work_presentations_and_emotions}
As stated by \citeA{derrico_tracking_2019} great speeches or presentations depend significantly on if the presenter transmits enough emotions while transferring their information. When compared to emotionless presentations, presentations that leverage emotional experiences and elicit an emotional response from the audience are more likely to capture the audience's attention \cite{gallo_talk_2014}. Further facts prove that emotions are an important driver for engaging presentations and are a crucial enabler for successful knowledge transfer. Brain researchers have discovered that presented knowledge will be remembered more likely if the presenter communicates emotionally \cite{tyng_influences_2017}. Moreover, politicians frequently postulate wrath and despair in order to convey empathy and concern about the matter at hand \cite{derrico_tracking_2019}. Also, most people do not judge brands with facts and data but rely heavily on feelings and emotions they have about the brand \cite{damasio_descartes_2006}.

An exemplary study by \citeA{de_carolis_engaged_2019} researched a tool for emotion recognition from facial expressions during e-learnings and comes close to our developed tool concerning the facial emotion recognition part. By evaluating facial expressions, head movements, and gaze behavior from 5.5-hour video recordings, the authors were able to automatically quantify studentsâ€™ engagement. The information gathered was linked to a subjective evaluation of engagement based on a four-dimensional questionnaire: challenge, skill, engagement, and perceived learning. According to \citeA{de_carolis_engaged_2019}, the less anxious and the more relaxed students are, the more they evaluated a more engaged questionnaire. They also showed that the more excited and engaged students felt during a presentation, the more the emotional analysis perceived them like this.

Another study by \citeA{chen_towards_2014} analyzes besides nonverbal behaviors also the speech delivery, such as fluency, pronunciation, and prosody. Here the authors collected several presentations from different speakers, which then were evaluated by human raters in different dimensions concerning the felt engagement during the presentation. They found out that indeed machine learning algorithms on vocal analysis can be used to assess the performance of presentations. Nevertheless, this study did not explicitly take into account emotion recognition but shows that the way the presenter delivers his speech correlates with the effectiveness of presentations.

Our paper project differs from previous studies since there has not yet been a tool developed, which can recognize emotions from two input sources: facial and vocal expressions. With this tool at hand, and previous findings in literature that emotions are an important driver for successful presentations, future presenters could enhance their presentation style by exactly knowing the audiencesâ€™ current emotional state.
