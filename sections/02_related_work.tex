\section{Related Work}
\label{sec:related_work}
In the following sections, a literature review is conducted. The first two parts describe the current scientific approaches to measure emotions for both faces and voices using deep neural networks. The third part explains the relationship between the quality of presentations and emotions.

\subsection{Facial Emotion Recognition}
\label{subsec:related_work_facial_emotion_recognition}
Facial Emotion Recognition (FER) is a subarea of computer vision and deals with the prediction of human emotional states using facial mimics and expressions in images, moving pictures or videos \cite{jain_extended_2019}.
According to \citeA{rosler_reducing_2021} FER literature and research suggest firstly, two separate ways of feature generation: manually or automatically through a deep neural network. Secondly, they outline the differentiation of their underlying emotional model, which is either based on discrete emotional states or on continuous dimensions \cite{rosler_reducing_2021}. Five fundamental discrete emotional states, for example identified by \citeA{ekman_universal_1997}, are happiness, sadness, fear / surprise, disgust and anger. There is several different literature recognizing other fundamental emotional states, and thus a set definition is not available. Also, the continuous dimensions are defined divergently. Given an instance, \citeA{mollahosseini_affectnet_2017} use two or three dimensions to explain emotions, with valence or pleasantness as one dimension and arousal or activation as the other.

In this work we will focus on leveraging discrete emotional states, since it is more maturely researched and more suitable to generate features using deep learning based approaches. Considering the amount of developments of recent years in deep learning, we concentrate on using the available variety of deep neural networks as the most appropriate technique in FER \cite{jain_extended_2019}. We could also focus on FER approaches that use handcrafted features and which are according to \citeA{ko_brief_2018} often deployed in the following three steps. First, face and facial component detection from the input image. Second, spatial and temporal feature extraction. And third, expression classification by e.g. Support Vector Machine or Random Forests to recognize emotional states. Although this manual way often leads to accurate results and requires less computational resources, many researches, such as \citeA{jung_joint_2015}, showed the superiority of deep learning algorithms with for example CNNs over handcrafted approaches.

One of the main advantages of CNNs and other neural networks is the possibility to automatically learn features from the input images, which is called ``end-to-end'' learning \cite{ko_brief_2018}. This Moody Prototype also uses a CNN implementation via the \texttt{face-api.js} library, which is described further in Section~\ref{subsec:method_facial_emotion_recognition}. These in the context of facial emotion recognition most used CNNs operate and process images, as the name discloses, ``convolutionally'' and can take spatial information into consideration \cite{ko_brief_2018, rosler_reducing_2021}. Thus, we utilize a CNN to recognize the emotional states of \citeA{ekman_universal_1997}: happy, sad, fearful, angry, surprised and disgusted. Additionally, since audiences listen and their faces appear often simply neutrally, we added another emotional state to our recognition model, called ``neutral''.
