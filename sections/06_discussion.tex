\section{Discussion}
\label{sec:Discussion}
In this seminar paper, we describe how we built our \url{moody.digital} web application, and how it recognizes face emotions with \texttt{face-api.js} and voice emotions with our self-built AlexNet. The web app is an extension of the existing web app from the previous semester of the COINs seminar at the University of Cologne in collaboration with the University of Bamberg and the University of Applied Science and Arts in Lucerne. The previous project already built an application that was able to track face emotions live during virtual meetings. With our additions, the app has gained further features. On the one hand side, it is now able to recognize live voice emotions in addition to face emotions. On the other hand side, emotions are displayed live during the meeting in an emotion roller-coaster. In addition to the live components, the data is also stored in a database for posthoc analysis. In this way, one can look at all the emotions of the meetings that have already been completed and evaluate them as one wishes. The feedback by the audience can also be visualized and analyzed. In our Zoom meetings during the COINs seminars, we were able to determine a first correlation between the face and voice emotions (cf. Figure \ref{fig:moody_statistics_screenshot}). To draw a meaningful conclusion, further video meetings must be analyzed with the tool. However, based on the accuracy of the emotion recognition models, we have designed an app that recognizes these with high accuracy and is therefore capable of such analyses. 
With this application, we have built a tool with which future presenters can improve their style of presentation and get more audience with it. It can also be used for further research purposes to gain more insights into presentations.
