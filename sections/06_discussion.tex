\section{Discussion}
\label{sec:Discussion}
In this seminar paper we describe how we built our moody.digital web application, and how it recognises face emotion with the face.api, and voice emotion with our self-built AlexNet. The web app is an extension of the existing web app from the previous semester of the COINs seminar at the University of Cologne in collaboration with the University of Bamberg and the University of Applied Science and Arts in Lucerne. The previous project already built an application that was able to track the face emotion live during virtual meetings. With our additions, the app has gained further features. For one thing, it is now able to recognise live voice emotion in addition to face emotion. Both emotions are displayed live during the meeting in an emotion roller-coaster. In addition to the live components, the data is also stored in the database for post analysis. In this way, you can look at all the emotions of the meetings that have already been completed and evaluate them as you wish. The feedback from the audience can also be viewed and analysed.In our Zoom meetings during the COINs seminars, we were able to determine the first correlation between the face and the voice emotions (cf. Figure \ref{fig:moody_statistics_screenshot}). In order to draw a meaningful conclusion, further video meetings must be analysed with the tool. However, based on the accuracy of the emotion recognition models, we have designed an app that recognises these with a high accuracy and is therefore capable of such analyses. 
With this application we have built a tool, with which future presenters can improve their style of presentation and get more audience with it. It can also be used for further research purposes to gain more insights about presentations.
