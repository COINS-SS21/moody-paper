
@article{rosler_reducing_2021,
	title = {Reducing Videoconferencing Fatigue through Facial Emotion Recognition},
	volume = {13},
	pages = {126},
	number = {5},
	journaltitle = {Future Internet},
	shortjournal = {Future Internet},
	author = {Rößler, Jannik and Sun, Jiachen and Gloor, Peter},
	date = {2021},
	note = {Publisher: Multidisciplinary Digital Publishing Institute}
}

@article{jain_extended_2019,
	title = {Extended deep neural network for facial emotion recognition},
	volume = {120},
	issn = {0167-8655},
	pages = {69--74},
	journaltitle = {Pattern Recognition Letters},
	shortjournal = {Pattern Recognition Letters},
	author = {Jain, Deepak Kumar and Shamsolmoali, Pourya and Sehdev, Paramjit},
	date = {2019},
	note = {Publisher: Elsevier}
}

@article{ekman_universal_1997,
	title = {Universal facial expressions of emotion},
	pages = {27--46},
	journaltitle = {Segerstrale U, P. Molnar P, eds. Nonverbal communication: Where nature meets culture},
	shortjournal = {Segerstrale U, P. Molnar P, eds. Nonverbal communication: Where nature meets culture},
	author = {Ekman, Paul and Keltner, Dacher},
	date = {1997}
}

@article{livingstone_ryerson_2018,
	title = {The Ryerson Audio-Visual Database of Emotional Speech and Song ({RAVDESS}): A dynamic, multimodal set of facial and vocal expressions in North American English},
	volume = {13},
	issn = {1932-6203},
	pages = {e0196391},
	number = {5},
	journaltitle = {{PloS} one},
	shortjournal = {{PloS} one},
	author = {Livingstone, Steven R and Russo, Frank A},
	date = {2018},
	note = {Publisher: Public Library of Science San Francisco, {CA} {USA}}
}

@article{delle-vigne_subclinical_2014,
	title = {Subclinical alexithymia modulates early audio-visual perceptive and attentional event-related potentials},
	volume = {8},
	issn = {1662-5161},
	pages = {106},
	journaltitle = {Frontiers in Human Neuroscience},
	shortjournal = {Frontiers in Human Neuroscience},
	author = {Delle-Vigne, Dyna and Kornreich, Charles and Verbanck, Paul and Campanella, Salvatore},
	date = {2014},
	note = {Publisher: Frontiers}
}

@article{zvyagintsev_attention_2013,
	title = {Attention and multisensory integration of emotions in schizophrenia},
	volume = {7},
	issn = {1662-5161},
	pages = {674},
	journaltitle = {Frontiers in human neuroscience},
	shortjournal = {Frontiers in human neuroscience},
	author = {Zvyagintsev, Mikhail and Parisi, Carmen and Chechko, Natalia and Nikolaev, Andrey R and Mathiak, Klaus},
	date = {2013},
	note = {Publisher: Frontiers}
}

@inproceedings{derrico_tracking_2019,
	title = {Tracking a leader’s humility and its emotions from body, face and voice},
	volume = {17},
	isbn = {2405-6456},
	eventtitle = {Web Intelligence},
	pages = {63--74},
	publisher = {{IOS} Press},
	author = {D’Errico, Francesca and Poggi, Isabella},
	date = {2019},
	note = {Issue: 1}
}

@article{zeng_emoco_2019,
	title = {{EmoCo}: Visual analysis of emotion coherence in presentation videos},
	volume = {26},
	issn = {1077-2626},
	pages = {927--937},
	number = {1},
	journaltitle = {{IEEE} transactions on visualization and computer graphics},
	shortjournal = {{IEEE} transactions on visualization and computer graphics},
	author = {Zeng, Haipeng and Wang, Xingbo and Wu, Aoyu and Wang, Yong and Li, Quan and Endert, Alex and Qu, Huamin},
	date = {2019},
	note = {Publisher: {IEEE}}
}

@article{swain_databases_2018,
	title = {Databases, features and classifiers for speech emotion recognition: a review},
	volume = {21},
	issn = {1572-8110},
	pages = {93--120},
	number = {1},
	journaltitle = {International Journal of Speech Technology},
	shortjournal = {International Journal of Speech Technology},
	author = {Swain, Monorama and Routray, Aurobinda and Kabisatpathy, Prithviraj},
	date = {2018},
	note = {Publisher: Springer}
}

@inproceedings{haq_speaker-dependent_2009,
	title = {Speaker-dependent audio-visual emotion recognition.},
	eventtitle = {{AVSP}},
	pages = {53--58},
	author = {Haq, Sanaul and Jackson, Philip {JB} and Edge, J},
	date = {2009}
}

@article{mollahosseini_affectnet_2017,
	title = {Affectnet: A database for facial expression, valence, and arousal computing in the wild},
	volume = {10},
	issn = {1949-3045},
	pages = {18--31},
	number = {1},
	journaltitle = {{IEEE} Transactions on Affective Computing},
	shortjournal = {{IEEE} Transactions on Affective Computing},
	author = {Mollahosseini, Ali and Hasani, Behzad and Mahoor, Mohammad H},
	date = {2017},
	note = {Publisher: {IEEE}}
}

@article{ko_brief_2018,
	title = {A brief review of facial emotion recognition based on visual information},
	volume = {18},
	pages = {401},
	number = {2},
	journaltitle = {sensors},
	shortjournal = {sensors},
	author = {Ko, Byoung Chul},
	date = {2018},
	note = {Publisher: Multidisciplinary Digital Publishing Institute}
}

@inproceedings{jung_joint_2015,
	title = {Joint fine-tuning in deep neural networks for facial expression recognition},
	eventtitle = {Proceedings of the {IEEE} international conference on computer vision},
	pages = {2983--2991},
	author = {Jung, Heechul and Lee, Sihaeng and Yim, Junho and Park, Sunjeong and Kim, Junmo},
	date = {2015}
}

@inproceedings{de_carolis_engaged_2019,
	title = {“Engaged Faces”: Measuring and Monitoring Student Engagement from Face and Gaze Behavior},
	eventtitle = {{IEEE}/{WIC}/{ACM} International Conference on Web Intelligence-Companion Volume},
	pages = {80--85},
	author = {De Carolis, Berardina and D'Errico, Francesca and Macchiarulo, Nicola and Palestra, Giuseppe},
	date = {2019}
}

@article{pichora-fuller_toronto_2020,
	title = {Toronto emotional speech set ({TESS})},
	url = {https://dataverse.scholarsportal.info/dataset.xhtml?persistentId=doi:10.5683/SP2/E8H2MF},
	doi = {10.5683/SP2/E8H2MF},
	abstract = {These stimuli were modeled on the Northwestern University Auditory Test No. 6 ({NU}-6; Tillman \& Carhart, 1966). A set of 200 target words were spoke...},
	author = {Pichora-Fuller, M. Kathleen and Dupuis, Kate},
	urldate = {2021-07-14},
	date = {2020-02-13},
	langid = {english},
	note = {Publisher: Scholars Portal Dataverse
Type: dataset},
	file = {Snapshot:/Users/kiliankaraus/Zotero/storage/7RHIAKKP/dataset.html:text/html}
}

@article{zhou_emotional_2021,
	title = {Emotional Voice Conversion: Theory, Databases and {ESD}},
	url = {http://arxiv.org/abs/2105.14762},
	shorttitle = {Emotional Voice Conversion},
	abstract = {In this paper, we first provide a review of the state-of-the-art emotional voice conversion research, and the existing emotional speech databases. We then motivate the development of a novel emotional speech database ({ESD}) that addresses the increasing research need. With this paper, the {ESD} database is now made available to the research community. The {ESD} database consists of 350 parallel utterances spoken by 10 native English and 10 native Chinese speakers and covers 5 emotion categories (neutral, happy, angry, sad and surprise). More than 29 hours of speech data were recorded in a controlled acoustic environment. The database is suitable for multi-speaker and cross-lingual emotional voice conversion studies. As case studies, we implement several state-of-the-art emotional voice conversion systems on the {ESD} database. This paper provides a reference study on {ESD} in conjunction with its release.},
	journaltitle = {{arXiv}:2105.14762 [cs]},
	author = {Zhou, Kun and Sisman, Berrak and Liu, Rui and Li, Haizhou},
	urldate = {2021-07-14},
	date = {2021-05-31},
	eprinttype = {arxiv},
	eprint = {2105.14762},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/kiliankaraus/Zotero/storage/A5K8CUJH/Zhou et al. - 2021 - Emotional Voice Conversion Theory, Databases and .pdf:application/pdf;arXiv.org Snapshot:/Users/kiliankaraus/Zotero/storage/5CGL2WXU/2105.html:text/html}
}

@inproceedings{james_open_2018,
	title = {An Open Source Emotional Speech Corpus for Human Robot Interaction Applications},
	doi = {10.21437/Interspeech.2018-1349},
	pages = {2768--2772},
	author = {James, Jesin and Tian, Li and Watson, Catherine},
	date = {2018-09-02},
	file = {Full Text PDF:/Users/kiliankaraus/Zotero/storage/G5JDXG7M/James et al. - 2018 - An Open Source Emotional Speech Corpus for Human R.pdf:application/pdf}
}

@article{burkhardt_database_2005,
	title = {A Database of German Emotional Speech},
	abstract = {The article describes a database of emotional speech. Ten actors (5 female and 5 male) simulated the emotions, producing 10 German utterances (5 short and 5 longer sentences) which could be used in everyday communication and are interpretable in all applied emotions.},
	pages = {4},
	author = {Burkhardt, Felix and Paeschke, A and Rolfes, M and Sendlmeier, W and Weiss, B},
	date = {2005},
	langid = {english},
	file = {Burkhardt et al. - 2005 - A Database of German Emotional Speech.pdf:/Users/kiliankaraus/Zotero/storage/SCD7C9H7/Burkhardt et al. - 2005 - A Database of German Emotional Speech.pdf:application/pdf}
}

@misc{onnx_runtime_developers_onnx_2021,
	title = {{ONNX} Runtime},
	url = {https://onnxruntime.ai/},
	author = {{ONNX Runtime developers}},
	date = {2021}
}

@article{kong_panns_2020,
	title = {Panns: Large-scale pretrained audio neural networks for audio pattern recognition},
	volume = {28},
	issn = {2329-9290},
	pages = {2880--2894},
	journaltitle = {{IEEE}/{ACM} Transactions on Audio, Speech, and Language Processing},
	shortjournal = {{IEEE}/{ACM} Transactions on Audio, Speech, and Language Processing},
	author = {Kong, Qiuqiang and Cao, Yin and Iqbal, Turab and Wang, Yuxuan and Wang, Wenwu and Plumbley, Mark D},
	date = {2020},
	note = {Publisher: {IEEE}}
}